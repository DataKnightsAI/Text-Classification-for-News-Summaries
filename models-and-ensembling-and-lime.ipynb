{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Models and Ensembling Methods + Interpretability with LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas\n",
    "from nltk import WordPunctTokenizer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import sqlite3\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn import svm\n",
    "from itertools import cycle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import lime.lime_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_FEATURE_SIZE = 300\n",
    "N_CLASSES = 4\n",
    "RANDOM_STATE = 123\n",
    "N_FOLDS = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Read in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Load raw train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Load in the data from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbconn = sqlite3.connect('./data/cleanedtraintest_v2.db')\n",
    "train_data_df = pandas.read_sql_query(\n",
    "    'SELECT category, content_cleaned FROM train_data', dbconn)\n",
    "test_data_df = pandas.read_sql_query(\n",
    "    'SELECT category, content_cleaned FROM test_data', dbconn)\n",
    "dbconn.commit()\n",
    "dbconn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Check the if the data was loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>content_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>wall street seeing green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>private investment firm carlyle group reputati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>soaring crude prices plus economy outlook earn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>authorities halted oil main pipeline southern ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>tearaway world oil prices toppling records str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>1</td>\n",
       "      <td>pakistani president pervez musharraf said stay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>2</td>\n",
       "      <td>red sox general manager theo epstein acknowled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>2</td>\n",
       "      <td>miami dolphins put courtship lsu coach nick sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>2</td>\n",
       "      <td>pittsburgh ny giants time line steelers record...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>2</td>\n",
       "      <td>vince carter traded toronto raptors new jersey...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                    content_cleaned\n",
       "0              3                           wall street seeing green\n",
       "1              3  private investment firm carlyle group reputati...\n",
       "2              3  soaring crude prices plus economy outlook earn...\n",
       "3              3  authorities halted oil main pipeline southern ...\n",
       "4              3  tearaway world oil prices toppling records str...\n",
       "...          ...                                                ...\n",
       "119995         1  pakistani president pervez musharraf said stay...\n",
       "119996         2  red sox general manager theo epstein acknowled...\n",
       "119997         2  miami dolphins put courtship lsu coach nick sa...\n",
       "119998         2  pittsburgh ny giants time line steelers record...\n",
       "119999         2  vince carter traded toronto raptors new jersey...\n",
       "\n",
       "[120000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>content_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>unions representing workers turner newall say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>toronto canada rocketeers competing million an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>company founded chemistry researcher universit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>barely dawn mike fitzpatrick starts shift blur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>southern california agency went emissions bovi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>1</td>\n",
       "      <td>ukrainian presidential candidate viktor yushch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <td>2</td>\n",
       "      <td>supply attractive pitching options dwindling d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597</th>\n",
       "      <td>2</td>\n",
       "      <td>like roger clemens almost exactly eight years ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>3</td>\n",
       "      <td>singapore doctors united states warned painkil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7599</th>\n",
       "      <td>3</td>\n",
       "      <td>ebay plans buy apartment home rental service m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                    content_cleaned\n",
       "0            3  unions representing workers turner newall say ...\n",
       "1            4  toronto canada rocketeers competing million an...\n",
       "2            4  company founded chemistry researcher universit...\n",
       "3            4  barely dawn mike fitzpatrick starts shift blur...\n",
       "4            4  southern california agency went emissions bovi...\n",
       "...        ...                                                ...\n",
       "7595         1  ukrainian presidential candidate viktor yushch...\n",
       "7596         2  supply attractive pitching options dwindling d...\n",
       "7597         2  like roger clemens almost exactly eight years ...\n",
       "7598         3  singapore doctors united states warned painkil...\n",
       "7599         3  ebay plans buy apartment home rental service m...\n",
       "\n",
       "[7600 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Train & Test data where x is the predictor features, y is the predicted feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data_df.content_cleaned\n",
    "y_train = label_binarize(train_data_df.category, classes=range(1, N_CLASSES + 1))\n",
    "\n",
    "x_test = test_data_df.content_cleaned\n",
    "y_test = label_binarize(test_data_df.category, classes=range(1, N_CLASSES + 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Load word2vec data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Load word2vec feature arrays from .npz files\n",
    " load dict of arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.43092448  0.50092196  0.08331972 ...  1.3914201   1.2953259\n",
      "  -1.8574607 ]\n",
      " [-0.10783155 -0.35169265  0.90062636 ... -0.38979718  0.13664657\n",
      "   0.5066641 ]\n",
      " [-1.0086536  -0.29255652 -0.7550053  ... -0.18521406  0.7896786\n",
      "  -0.23576818]\n",
      " ...\n",
      " [-0.02566049  0.23409443 -0.8595321  ... -0.05427613 -0.89297265\n",
      "  -0.09055152]\n",
      " [-0.6081659   0.42683512 -0.9105423  ... -0.06156884 -0.40654626\n",
      "   0.07195716]\n",
      " [-1.0819023  -0.04211196 -0.16453283 ... -0.40625843 -0.13644677\n",
      "  -0.0066904 ]]\n",
      "[[-0.02657197 -1.0014614  -0.035705   ...  0.48677683  0.3947945\n",
      "  -0.9894788 ]\n",
      " [-0.54866743 -1.3801866   0.66031504 ... -0.4012159   0.6803215\n",
      "   0.92033225]\n",
      " [ 0.11171789  0.3781767  -0.26057357 ... -0.5006595   0.13674003\n",
      "   0.10530389]\n",
      " ...\n",
      " [-0.46190766  0.7501185  -0.20256642 ... -0.32613838  0.09363924\n",
      "   0.46578252]\n",
      " [-0.023529   -0.33200815 -0.63418424 ... -0.46149412  0.39634904\n",
      "  -0.46027517]\n",
      " [-0.25388533 -0.6177681   0.9628809  ... -0.66557425 -0.1068292\n",
      "  -0.64577085]]\n"
     ]
    }
   ],
   "source": [
    "w2v_train_features_array_dict = numpy.load(\n",
    "    './data/word2vec-train-features-120000-min5dim300.npz')\n",
    "w2v_test_features_array_dict = numpy.load(\n",
    "    './data/word2vec-test-features-120000-min5dim300.npz')\n",
    "# extract the first array from train\n",
    "data = w2v_train_features_array_dict['arr_0']\n",
    "# print the array\n",
    "print(data)\n",
    "# extract the first array from test\n",
    "data = w2v_test_features_array_dict['arr_0']\n",
    "# print the array\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Load word2vec model trained key vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_train = KeyedVectors.load(\n",
    "    './data/custom-trained-word2vec-120000-min5dim300.kv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Get the word2vec data back into usable form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = WordPunctTokenizer()\n",
    "tokenized_corpus_train = [wpt.tokenize(document) for document in x_train]\n",
    "tokenized_corpus_test = [wpt.tokenize(document) for document in x_test]\n",
    "\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    feature_vector = numpy.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = numpy.add(feature_vector, model[word])\n",
    "\n",
    "    if nwords:\n",
    "        feature_vector = numpy.divide(feature_vector, nwords)\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "            for tokenized_sentence in corpus]\n",
    "    return numpy.array(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Obtain document level embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_feature_array_train = averaged_word_vectorizer(corpus=tokenized_corpus_train,\n",
    "    model=w2v_model_train, num_features=W2V_FEATURE_SIZE)\n",
    "w2v_feature_array_test = averaged_word_vectorizer(corpus=tokenized_corpus_test,\n",
    "    model=w2v_model_train, num_features=W2V_FEATURE_SIZE)\n",
    "\n",
    "x_train_w2v = pandas.DataFrame(w2v_feature_array_train)\n",
    "x_test_w2v = pandas.DataFrame(w2v_feature_array_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Sample down for speed, for now. (use when testing)\n",
    " x_train_w2v = x_train_w2v.sample(\n",
    "     n = 3000, replace = False, random_state = RANDOM_STATE\n",
    " )\n",
    " y_train = train_data_df.category.sample(\n",
    "     n = 3000, replace = False, random_state = RANDOM_STATE\n",
    " )\n",
    " y_train = label_binarize(y_train, classes=range(1, N_CLASSES + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Build Models\n",
    " ### SVM Model Building Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm(x_train, y_train):\n",
    "    classifier = OneVsRestClassifier(svm.LinearSVC(random_state=RANDOM_STATE))\n",
    "    classifier.fit(x_train, y_train)\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Logistic Regression Model Building Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logreg(x_train, y_train):\n",
    "    classifier = OneVsRestClassifier(LogisticRegression(random_state=RANDOM_STATE))\n",
    "    classifier.fit(x_train, y_train)\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Naive Bayes Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nb(x_train, y_train):\n",
    "    classifier = OneVsRestClassifier(GaussianNB())\n",
    "    classifier.fit(x_train, y_train)\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Decision Trees Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dectree(x_train, y_train):\n",
    "    classifier = OneVsRestClassifier(tree.DecisionTreeClassifier())\n",
    "    classifier.fit(x_train, y_train)\n",
    "    return classifier \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Functions to calculate scores and to plot them\n",
    " Calculate, then plot the Precision, Recall, Average Precision, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prf1_calc(classifier, algo_name, n_classes, x_test, y_test):\n",
    "    # Get the decision function from the classifier\n",
    "    if algo_name == 'SVM':\n",
    "        y_score = classifier.decision_function(x_test)\n",
    "    else:\n",
    "        y_score = classifier.predict_proba(x_test)\n",
    "    y_pred = classifier.predict(x_test)\n",
    "\n",
    "    # The average precision score in multi-label settings\n",
    "    # For each class\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    average_f1 = dict()\n",
    "    average_precision = dict()\n",
    "    mcc = dict()\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n",
    "                                                            y_score[:, i])\n",
    "        average_precision[i] = average_precision_score(y_test[:, i], y_score[:, i])\n",
    "        average_f1[i] = f1_score(y_test[:, i], y_pred[:, i])\n",
    "        mcc[i] = matthews_corrcoef(y_test[:, i], y_pred[:, i])\n",
    "\n",
    "    # A \"micro-average\": quantifying score on all classes jointly\n",
    "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(),\n",
    "        y_score.ravel())\n",
    "    average_precision[\"micro\"] = average_precision_score(y_test, y_score,\n",
    "                                                        average=\"micro\")\n",
    "    average_f1['micro'] = f1_score(y_test, y_pred, average='micro')\n",
    "    mcc['micro'] = sum(mcc.values())/4\n",
    "\n",
    "    # Plot the data\n",
    "    prf1_plot(precision, recall, average_precision, algo_name, n_classes)\n",
    "\n",
    "    # Return all metrics\n",
    "    results = pandas.DataFrame()\n",
    "\n",
    "    for k in average_precision.keys():\n",
    "        results.at[algo_name, f'P-R {k}'] = numpy.round(average_precision[k], 3)\n",
    "        results.at[algo_name, f'F1 {k}'] = numpy.round(average_f1[k], 3)\n",
    "        results.at[algo_name, f'MCC {k}'] = numpy.round(mcc[k], 3)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Function to Plot Precision, Recall, F1\n",
    "def prf1_plot(precision, recall, average_precision, algo_name, n_classes):\n",
    "    print(algo_name)\n",
    "    print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "        .format(average_precision[\"micro\"]))\n",
    "\n",
    "    # Plot the micro-averaged Precision-Recall curve\n",
    "    plt.figure()\n",
    "    plt.step(recall['micro'], precision['micro'], where='post')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title(\n",
    "        'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n",
    "        .format(average_precision[\"micro\"]))\n",
    "    \n",
    "    # Plot Precision-Recall curve for each class and iso-f1 curves\n",
    "    # setup plot details\n",
    "    colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n",
    "\n",
    "    plt.figure(figsize=(7, 8))\n",
    "    f_scores = numpy.linspace(0.2, 0.8, num=4)\n",
    "    lines = []\n",
    "    labels = []\n",
    "    for f_score in f_scores:\n",
    "        x = numpy.linspace(0.01, 1)\n",
    "        y = f_score * x / (2 * x - f_score)\n",
    "        l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "        plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "    lines.append(l)\n",
    "    labels.append('iso-f1 curves')\n",
    "    l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('micro-average Precision-recall (area = {0:0.2f})'\n",
    "                ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "        lines.append(l)\n",
    "        labels.append('Precision-recall for class {0} (area = {1:0.2f})'\n",
    "                    ''.format(i, average_precision[i]))\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.subplots_adjust(bottom=0.25)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "    plt.legend(lines, labels, loc=(0, -.5), prop=dict(size=14))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Run the Base Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Run SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = run_svm(x_train_w2v, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Run Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = run_logreg(x_train_w2v, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Run Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = run_nb(x_train_w2v, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Run Decision Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dectree_model = run_dectree(x_train_w2v, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Get the scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Initialize the dataframe to keep track of the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pandas.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Precision, Recall, Avg. Precision for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = scores.append(prf1_calc(svm_model, 'SVM', N_CLASSES, x_test_w2v, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Precision, Recall, Avg. Precision for LOG REG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = scores.append(prf1_calc(logreg_model, 'LOGREG', N_CLASSES, x_test_w2v, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Precision, Recall, Avg. Precision for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = scores.append(prf1_calc(nb_model, 'NB', N_CLASSES, x_test_w2v, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Precision, Recall, Avg. Precision for Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = scores.append(prf1_calc(dectree_model, 'DT', N_CLASSES, x_test_w2v, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Look at Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Create model list to iterate through for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = OneVsRestClassifier(GaussianNB())\n",
    "sv = OneVsRestClassifier(svm.LinearSVC(random_state=RANDOM_STATE))\n",
    "lreg = OneVsRestClassifier(LogisticRegression(random_state=RANDOM_STATE))\n",
    "dtree = OneVsRestClassifier(tree.DecisionTreeClassifier())\n",
    "\n",
    "model_list = [gnb, sv, lreg, dtree]\n",
    "model_namelist = ['Gaussian Naive Bayes',\n",
    "                  'SVM/Linear SVC', \n",
    "                  'Logistic Regression', \n",
    "                  'Decision Tree']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Make scoring metrics to pass cv function through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'precision': make_scorer(precision_score, average='micro'), \n",
    "           'recall': make_scorer(recall_score, average='micro'), \n",
    "           'f1': make_scorer(f1_score, average='micro'),\n",
    "           'roc_auc': make_scorer(roc_auc_score, average='micro'),\n",
    "           # 'mcc': make_scorer(matthews_corrcoef) <- cannot support multi-label\n",
    "          }\n",
    "\n",
    "cv_result_entries = []\n",
    "i = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Loop cross validation through various models and generate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mod in model_list:\n",
    "    metrics = cross_validate(\n",
    "        mod,\n",
    "        x_train_w2v,\n",
    "        y_train,\n",
    "        cv=N_FOLDS,\n",
    "        scoring = scoring,\n",
    "        return_train_score=False,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    for key in metrics.keys():\n",
    "        for fold_index, score in enumerate(metrics[key]):\n",
    "            cv_result_entries.append((model_namelist[i], fold_index, key, score))\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Save the cv results to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_df = pandas.DataFrame(cv_result_entries)\n",
    "cv_results_df.columns = ['algo', 'cv fold', 'metric', 'value']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Plot cv results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_name, metric in zip(['fit_time',\n",
    "                                'test_precision',\n",
    "                                'test_recall',\n",
    "                                'test_f1',\n",
    "                                'test_roc_auc'],\n",
    "                                ['Fit Time',\n",
    "                                'Precision',\n",
    "                                'Recall',\n",
    "                                'F1 Score',\n",
    "                                'ROC AUC']):\n",
    "    sns.boxplot(x='algo', y='value', #hue='algo',\n",
    "        data=cv_results_df[cv_results_df.metric.eq(f'{metric_name}')])\n",
    "    sns.stripplot(x='algo', y = 'value', \n",
    "        data = cv_results_df[cv_results_df.metric.eq(f'{metric_name}')],\n",
    "        size = 5, linewidth = 1)\n",
    "    plt.title(f'{metric} Algo Comparison', fontsize=12)\n",
    "    plt.xlabel('Algorithm', fontsize=12)\n",
    "    plt.ylabel(f'{metric}', fontsize=12)\n",
    "    plt.xticks([0, 1, 2, 3, 4])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Misclassification Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for model in model_list:\n",
    "    plt.figure()\n",
    "    plot_learning_curves(x_train_w2v, y_train, x_test_w2v, y_test, model)\n",
    "    plt.title('Learning Curve for ' + model_namelist[i], fontsize=14)\n",
    "    plt.xlabel('Training Set Size (%)', fontsize=12)\n",
    "    plt.ylabel('Misclassification Error', fontsize=12)\n",
    "    plt.show()\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = []\n",
    "for model in model_list:\n",
    "    y_test_pred.append(model.predict(x_test_w2v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "i=0\n",
    "for _ in model_list:\n",
    "    cm = confusion_matrix(numpy.argmax(y_test, axis=1),\n",
    "                          numpy.argmax(y_test_pred[i], axis=1))\n",
    "    cm_df = pandas.DataFrame(cm, index = CLASSES, columns = CLASSES)\n",
    "    cm_df.index.name = 'Actual'\n",
    "    cm_df.columns.name = 'Predicted'\n",
    "    plt.title('Confusion Matrix for ' + model_namelist[i], fontsize=14)\n",
    "    sns.heatmap(cm_df, annot=True, fmt='.6g', annot_kws={\"size\": 10}, cmap='Reds')\n",
    "    plt.show()\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## HYPER PARAMETER TUNING BY HYPEROPT (not working!!!)\n",
    " from hyperopt import STATUS_OK\n",
    " N_FOLDS = 5\n",
    " Objective Function\n",
    " def objective(params, n_folds = N_FOLDS):\n",
    "     cv_results = cross_validate(OneVsRestClassifier(GaussianNB()),\n",
    "         x_train_w2v,\n",
    "         y_train,\n",
    "         cv = n_folds,\n",
    "         fit_params= params,\n",
    "         scoring = {'f1': make_scorer(f1_score, average='micro')},\n",
    "         return_train_score=False,\n",
    "         n_jobs=-1\n",
    "     )\n",
    "     # Extract the best score\n",
    "     best_score = max(cv_results['test_f1'])\n",
    "     # Loss must be minimized\n",
    "     loss = 1 - best_score\n",
    "     # Dictionary with information for evaluation\n",
    "     return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    " Domain Space\n",
    " from hyperopt import hp\n",
    " space = {'estimator__var_smoothing': hp.uniform('estimator__var_smoothing',\n",
    "                           1.e+00, 1.e-09)}\n",
    " Optimization Algorithm\n",
    " from hyperopt import tpe\n",
    " tpe_algo = tpe.suggest\n",
    " Results History\n",
    " from hyperopt import Trials\n",
    " bayes_trials = Trials()\n",
    " Run the optimization\n",
    " from hyperopt import fmin\n",
    " from hyperopt import rand\n",
    " MAX_EVALS = 500\n",
    " params = space\n",
    " Optimize\n",
    " best = fmin(fn = objective, space = space, algo = tpe.suggest,\n",
    "             max_evals = 100, trials = bayes_trials)\n",
    " print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempx = x_train_w2v\n",
    "tempy = y_train\n",
    "x_train_w2v = x_train_w2v.sample(n = 3000, replace = False, random_state = RANDOM_STATE)\n",
    "y_train = train_data_df.category.sample(\n",
    "    n = 3000, replace = False, random_state = RANDOM_STATE\n",
    ")\n",
    "y_train = label_binarize(y_train, classes=range(1, N_CLASSES + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Hyper-parameter tuning with exhaustive Grid Search\n",
    " ### Tune hyperparameters for Gaussian Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_gnb = {'estimator__var_smoothing': [1.e-09, 1.e-08, 1.e-07, 1.e-06, 1.e-05,\n",
    "                                           1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00]\n",
    "}\n",
    "clf = GridSearchCV(estimator=gnb,\n",
    "                   param_grid=params_gnb,\n",
    "                   scoring='f1_micro',\n",
    "                   n_jobs=-1,\n",
    "                   cv=N_FOLDS,\n",
    "                   return_train_score=True\n",
    "                  )\n",
    "clf_res = clf.fit(x_train_w2v, y_train)\n",
    "print('Best Score: ', clf_res.best_score_)\n",
    "print('Best Params: ', clf_res.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Tune hyperparameters for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lreg = {\n",
    "    \"estimator__penalty\": ['l1', 'l2'],\n",
    "    \"estimator__C\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    #\"estimator__class_weight\":[{1:0.5, 0:0.5}, {1:0.4, 0:0.6},\n",
    "    #                           {1:0.6, 0:0.4}, {1:0.7, 0:0.3}],\n",
    "    \"estimator__solver\": [\"newton-cg\", \"sag\", \"saga\", \"lbfgs\"]\n",
    "}\n",
    "clf = GridSearchCV(estimator=lreg,\n",
    "                   param_grid=params_lreg,\n",
    "                   scoring='f1_micro',\n",
    "                   n_jobs=-1,\n",
    "                   cv=N_FOLDS,\n",
    "                   return_train_score=True\n",
    "                  )\n",
    "clf_res = clf.fit(x_train_w2v, y_train)\n",
    "print('Best score:', clf_res.best_score_)\n",
    "print('Best Params:', clf_res.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Tune hyperparameters for SVM (Linear SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_sv = {\n",
    "    \"estimator__penalty\":['l1', 'l2'],\n",
    "    \"estimator__tol\": [1.e-08, 1.e-07, 1.e-06, 1.e-05,\n",
    "                       1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00],\n",
    "    \"estimator__loss\":['hinge','squared_hinge'],\n",
    "    \"estimator__C\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "    #\"estimator__class_weight\":['None',{1:0.5, 0:0.5}, \n",
    "    #                           {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}],\n",
    "}\n",
    "clf = GridSearchCV(estimator=sv,\n",
    "                   param_grid=params_sv,\n",
    "                   scoring='f1_micro',\n",
    "                   n_jobs=-1,\n",
    "                   cv=N_FOLDS,\n",
    "                   return_train_score=False\n",
    "                  )\n",
    "clf_res = clf.fit(x_train_w2v, y_train)\n",
    "print('Best score:', clf_res.best_score_)\n",
    "print('Best Params:', clf_res.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Tune hyperparameters for Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dtree = {\n",
    "    \"estimator__splitter\":[\"best\", \"random\"],\n",
    "    \"estimator__min_samples_split\":range(1, 20, 1)\n",
    "}\n",
    "clf = GridSearchCV(estimator=dtree,\n",
    "                   param_grid=params_dtree,\n",
    "                   scoring='f1_micro',\n",
    "                   n_jobs=-1,\n",
    "                   cv=N_FOLDS,\n",
    "                   return_train_score=False\n",
    "                  )\n",
    "clf_res = clf.fit(x_train_w2v, y_train)\n",
    "print('Best score:', clf_res.best_score_)\n",
    "print('Best Params:', clf_res.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Conclusion:\n",
    " Apparently the best params are pretty much the default ones.\n",
    " The algorithms are already pretty smart about the defaults or can calculate them.\n",
    " This tuning these hyper-parameters might actually cause overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "              ('nb', GaussianNB()),\n",
    "              ('svm', svm.LinearSVC())\n",
    "             ]\n",
    "\n",
    "sclf = OneVsRestClassifier(StackingClassifier(\n",
    "    estimators=estimators, final_estimator=LogisticRegression())\n",
    ")\n",
    "\n",
    "metrics = cross_validate(\n",
    "    sclf,\n",
    "    x_train_w2v,\n",
    "    y_train,\n",
    "    cv=N_FOLDS,\n",
    "    scoring = scoring,\n",
    "    return_train_score=False,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "res = []\n",
    "for key in metrics.keys():\n",
    "    for fold_index, score in enumerate(metrics[key]):\n",
    "        res.append(('Stacking', fold_index, key, score))\n",
    "\n",
    "res_df = pandas.DataFrame.from_dict(res)\n",
    "res_df.columns = ['algo', 'cv fold', 'metric', 'value']\n",
    "cv_results_inc_ens = pandas.concat([cv_results_df, res_df])\n",
    "print(res_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sclf = OneVsRestClassifier(BaggingClassifier(\n",
    "    base_estimator=LogisticRegression())\n",
    ")\n",
    "\n",
    "metrics = cross_validate(\n",
    "    sclf,\n",
    "    x_train_w2v,\n",
    "    y_train,\n",
    "    cv=N_FOLDS,\n",
    "    scoring = scoring,\n",
    "    return_train_score=False,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "res = []\n",
    "for key in metrics.keys():\n",
    "    for fold_index, score in enumerate(metrics[key]):\n",
    "        res.append(('Bagging', fold_index, key, score))\n",
    "\n",
    "res_df = pandas.DataFrame.from_dict(res)\n",
    "res_df.columns = ['algo', 'cv fold', 'metric', 'value']\n",
    "cv_results_inc_ens = pandas.concat([cv_results_inc_ens, res_df])\n",
    "print(res_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "sclf = OneVsRestClassifier(AdaBoostClassifier(\n",
    "    random_state=RANDOM_STATE)\n",
    ")\n",
    "\n",
    "metrics = cross_validate(\n",
    "    sclf,\n",
    "    x_train_w2v,\n",
    "    y_train,\n",
    "    cv=N_FOLDS,\n",
    "    scoring = scoring,\n",
    "    return_train_score=False,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "res = []\n",
    "for key in metrics.keys():\n",
    "    for fold_index, score in enumerate(metrics[key]):\n",
    "        res.append(('AdaBoost', fold_index, key, score))\n",
    "\n",
    "res_df = pandas.DataFrame.from_dict(res)\n",
    "res_df.columns = ['algo', 'cv fold', 'metric', 'value']\n",
    "cv_results_inc_ens = pandas.concat([cv_results_inc_ens, res_df])\n",
    "print(res_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Plot cv results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_name, metric in zip(['fit_time',\n",
    "                                'test_precision',\n",
    "                                'test_recall',\n",
    "                                'test_f1',\n",
    "                                'test_roc_auc'],\n",
    "                                ['Fit Time',\n",
    "                                'Precision',\n",
    "                                'Recall',\n",
    "                                'F1 Score',\n",
    "                                'ROC AUC']):\n",
    "    sns.boxplot(x='algo', y='value', #hue='algo',\n",
    "        data=cv_results_inc_ens[cv_results_inc_ens.metric.eq(f'{metric_name}')])\n",
    "    sns.stripplot(x='algo', y = 'value', \n",
    "        data = cv_results_inc_ens[cv_results_inc_ens.metric.eq(f'{metric_name}')],\n",
    "        size = 5, linewidth = 1)\n",
    "    plt.title(f'{metric} Algo Comparison', fontsize=12)\n",
    "    plt.xlabel('Algorithm', fontsize=12)\n",
    "    plt.ylabel(f'{metric}', fontsize=12)\n",
    "    plt.xticks([0, 1, 2, 3, 4])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Save our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_inc_ens.to_csv('./data/cv-results-inc-ens.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## LIME for model interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=['World','Sports','Business','Tech/Sci']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Instantiate explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_explainer = lime.lime_tabular.LimeTabularExplainer(x_train_w2v.values,\n",
    "    mode='classification', class_names=class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Get explanations for: idx = Document #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 34\n",
    "tab_exp_lreg = tab_explainer.explain_instance(x_test_w2v.values[idx], lreg.predict_proba,\n",
    "    num_features=10, top_labels=1)\n",
    "print('Document id: %d' % idx)\n",
    "print('Predicted class =',\n",
    "    class_names[numpy.argmax(lreg.predict(numpy.array(x_test_w2v.values[idx]).reshape(1, -1)))])\n",
    "print('True class =', class_names[numpy.argmax(y_test[idx])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Get a text-only explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('\\n'.join(map(str, tab_exp_lreg.as_list(label=numpy.argmax(y_test[idx])))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Get a graphical explanation of the predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = tab_exp_lreg.as_pyplot_figure(label=numpy.argmax(y_test[idx]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Get a graphical explanation with class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_exp_lreg.show_in_notebook()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## References - Code sample sources disclaimer:\n",
    " Code for this project is either directly from (with some modification),\n",
    " or inspired by, but not limited to the following sources:\n",
    " - Respective documentation and examples from each used API's doc/guide website\n",
    " - Kelly Epley Naive Bayes:\n",
    "   https://towardsdatascience.com/naive-bayes-document-classification-in-python-e33ff50f937e\n",
    " - MLWhiz's excellent blogs about text classification and NLP:\n",
    "   https://mlwhiz.com/blog/2018/12/17/text_classification/\n",
    "   https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/\n",
    "   https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/\n",
    "   https://www.kaggle.com/mlwhiz/conventional-methods-for-quora-classification/\n",
    " - Christof Henkel preprocessing:\n",
    "   https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings\n",
    " - datanizing GmbH:\n",
    "   https://medium.com/@datanizing/modern-text-mining-with-python-part-1-of-5-introduction-cleaning-and-linguistics-647f9ec85b6a\n",
    " - Datacamp wordcloud:\n",
    "   https://www.datacamp.com/community/tutorials/wordcloud-python\n",
    " - Seaborn Pydata tutorials:\n",
    "   https://seaborn.pydata.org/introduction.html#intro-plot-customization\n",
    " - Dipanjan S's tutorials:\n",
    "   https://github.com/dipanjanS\n",
    " - Analytics Vidhya:\n",
    "   https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/\n",
    " - Jason Brownlee's Feature Selection For Machine Learning in Python\n",
    "   https://machinelearningmastery.com/feature-selection-machine-learning-python/\n",
    " - Susan Li's Multi-class text classification with Scikit-learn:\n",
    "   https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f\n",
    " - Vadim Smolyakov Ensemble Learning to Improve Machine Learning Results:\n",
    "   https://blog.statsbot.co/ensemble-learning-d1dcd548e936\n",
    " - Udacity course video on Youtube UD120:\n",
    "   https://www.youtube.com/watch?v=GdsLRKjjKLw\n",
    " - Hyperparameter Tuning with Hyperopt\n",
    "   https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a\n",
    " - Hyperparameter Tuning for Gaussian NB\n",
    "   https://www.quora.com/Can-the-prior-in-a-naive-Bayes-be-considered-a-hyperparameter-and-tuned-for-better-accuracy\n",
    " - Hyperparameter Tuning for Decision Trees\n",
    "   https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680\n",
    " - Lime tutorial\n",
    "   https://marcotcr.github.io/lime/tutorials/Lime%20-%20multiclass.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('env1': conda)",
   "language": "python",
   "name": "python37664bitenv1conda5d7aff6cf43e4682a679506634d57c11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
